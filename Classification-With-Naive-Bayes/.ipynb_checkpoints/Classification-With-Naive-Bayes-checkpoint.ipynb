{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Classification With Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, bernoulli\n",
    "from cv2 import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p1\n",
    "#Read the data using pandas. Skip first row. There are a total of 767 data-points.\n",
    "pid_df = pd.read_csv(\"pima-indians-diabetes.csv\", skiprows = [0], names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Class\"])\n",
    "\n",
    "#General classifier class.\n",
    "class Classifier(object):\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "    def train(self):\n",
    "        pass\n",
    "    def predict(self):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p1\n",
    "#naive Bayes classifier\n",
    "class NBC(Classifier):\n",
    "    def __init__(self, df=None, missing = [], train_df = None, test_df = None, distribution=\"Normal\"):\n",
    "        self.distribution = distribution\n",
    "        if df is not None:\n",
    "            super().__init__(*self.split(df))\n",
    "        else:\n",
    "            super().__init__(train_df, test_df)\n",
    "        self.missing = missing\n",
    "        \n",
    "    def train(self):\n",
    "        #calculate the prior probabilities.\n",
    "        p_prior = {}\n",
    "        for index, example in self.train_df.iterrows():\n",
    "            p_prior[example[\"Class\"]] = p_prior.get(example[\"Class\"], 0) + 1\n",
    "        assert sum(p_prior.values()) == len(self.train_df), \"failed to compute the prior\"\n",
    "        for key in p_prior:\n",
    "            p_prior[key] = p_prior[key]/len(self.train_df)\n",
    "        self.p_prior = p_prior\n",
    "        #calculate the likelihood normal distributions mean and std.\n",
    "        p_likelihood = {}\n",
    "        for col in list(self.train_df)[:-1]:\n",
    "            p_likelihood[col] = {}\n",
    "            for class_value in p_prior:\n",
    "                values = self.train_df.loc[self.train_df[\"Class\"] == class_value, col].values\n",
    "                if self.distribution == \"Normal\":\n",
    "                    #1.b Ignore 0 values to calculate the mean and std.\n",
    "                    if col in self.missing:     \n",
    "                        values = values[values != 0]\n",
    "                    mean, std = np.mean(values), np.std(values)\n",
    "                    p_likelihood[col][class_value] = np.array([mean, std]);\n",
    "                    #Bernoulli\n",
    "                elif self.distribution == \"Bernoulli\":\n",
    "                    p_likelihood[col][class_value] = np.sum(values)/len(values)\n",
    "        self.p_likelihood = p_likelihood\n",
    "        return self\n",
    "    \n",
    "    # Trying to avoid nested for loop to speed up the computation. Speed up more than 100 times!!!\n",
    "    def predict(self):\n",
    "        log_p_posterior = {class_value:np.log(np.array([self.p_prior[class_value] for _, _ in self.test_df.iterrows()])) for class_value in self.p_prior}\n",
    "        for col in list(self.test_df)[:-1]:\n",
    "            for class_value in self.p_prior:\n",
    "                values = self.test_df[col].values\n",
    "                if self.distribution == \"Normal\":\n",
    "                    mean, std = self.p_likelihood[col][class_value]\n",
    "                    #2 Only compute the pdf with non-zero std\n",
    "                    if std == 0:\n",
    "                        continue\n",
    "                    p = norm.pdf(values, mean, std)\n",
    "                elif self.distribution == \"Bernoulli\":\n",
    "                    p = bernoulli.pmf(values, self.p_likelihood[col][class_value])\n",
    "                #1.b Set p=1 for 0 feature values in testing examples to calculate the posterior. \n",
    "                if col in self.missing:  \n",
    "                    p = np.where(values != 0,p,1)\n",
    "                #2 Avoid log 0 p values\n",
    "                p = np.where(p != 0,p,1)\n",
    "                log_p_posterior[class_value] += np.log(p)\n",
    "        i = 0\n",
    "        self.res = {}\n",
    "        for index, example in self.test_df.iterrows():\n",
    "            self.res[index] = max(log_p_posterior, key=lambda x:log_p_posterior[x][i])\n",
    "            i += 1\n",
    "        return self\n",
    "        '''\n",
    "        for index, example in self.test_df.iterrows():\n",
    "            log_p_predict = {class_value:np.log(self.p_prior[class_value]) for class_value in self.p_prior}\n",
    "            for col in list(self.test_df)[:-1]:\n",
    "                for class_value in self.p_prior:\n",
    "                    #1.b Ignore 0 values in testing examples to calculate the posterior.\n",
    "                    if col in self.missing:\n",
    "                        if example[col] == 0:\n",
    "                            continue\n",
    "                    #2 only compute the pdf of existing distribution\n",
    "                    if self.p_likelihood[col][class_value] != None:\n",
    "                        p = self.p_likelihood[col][class_value].pdf(example[col])\n",
    "                        if p != 0:                                           \n",
    "                            log_p_predict[class_value] += np.log(p)\n",
    "            self.res[index] = max(log_p_predict, key=log_p_predict.get)\n",
    "        return self\n",
    "        '''\n",
    "    \n",
    "    def get_acc(self):\n",
    "        cnt = 0 \n",
    "        for index, example in self.test_df.iterrows():\n",
    "            cnt += 1 if example[\"Class\"] == self.res[index] else 0\n",
    "        return cnt/len(self.test_df)\n",
    "                        \n",
    "    # split the data, 20% for testing, rest for training.\n",
    "    def split(self, df):\n",
    "        msk = np.random.rand(len(df)) < 0.8\n",
    "        train = df[msk]\n",
    "        test = df[~msk]\n",
    "        return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.a\n",
      "0.7539772027863384\n",
      "1.b\n",
      "0.7431450604175607\n"
     ]
    }
   ],
   "source": [
    "#p1.a\n",
    "#calculate the average over 10 splits\n",
    "print(\"1.a\")\n",
    "num_splits = 10\n",
    "print(sum([NBC(df = pid_df).train().predict().get_acc() for _ in range(num_splits)])/num_splits)\n",
    "\n",
    "#p1.b\n",
    "print(\"1.b\")\n",
    "num_splits = 10\n",
    "print(sum([NBC(df = pid_df, missing = [\"BloodPressure\", \"SkinThickness\",\"BMI\", \"Age\"]).train().predict().get_acc() for _ in range(num_splits)])/num_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 400)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "#p2\n",
    "#load the data\n",
    "from mnist import MNIST\n",
    "mndata = MNIST(\"./python-mnist/data\")\n",
    "train_images, train_labels = mndata.load_training()\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels).reshape(len(train_labels), 1)\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels).reshape(len(test_labels), 1)\n",
    "\n",
    "#thresholding\n",
    "threshold = 127\n",
    "train_images = np.where(threshold >= train_images,0,1)\n",
    "test_images = np.where(threshold >= test_images,0,1)\n",
    "\n",
    "#stretch and resize\n",
    "def stretch(i):\n",
    "    i = i.reshape(28,28)\n",
    "    r_sum = np.sum(i, axis=1)\n",
    "    c_sum = np.sum(i, axis=0)\n",
    "    r_max,r_min = np.argwhere(r_sum>0)[0][0], np.argwhere(r_sum>0)[-1][0]\n",
    "    c_max,c_min = np.argwhere(c_sum>0)[0][0], np.argwhere(c_sum>0)[-1][0]\n",
    "    r_center,c_center = (r_max+r_min)//2, (c_max+c_min)//2\n",
    "    if r_center < 10:\n",
    "        r_center = 10\n",
    "    if c_center < 10:\n",
    "        c_center = 10\n",
    "    if r_center > 18:\n",
    "        r_center = 18\n",
    "    if c_center > 18:\n",
    "        c_center = 18\n",
    "    i = i[r_center-10:r_center+10,c_center-10:c_center+10]\n",
    "    return i.reshape(20*20,)\n",
    "train_stretched_images = []\n",
    "test_stretched_images = []\n",
    "for i in train_images:\n",
    "    train_stretched_images.append(stretch(i))\n",
    "for i in test_images:\n",
    "    test_stretched_images.append(stretch(i))\n",
    "train_stretched_images = np.array(train_stretched_images)\n",
    "test_stretched_images = np.array(test_stretched_images)\n",
    "\n",
    "#create dataframe using images\n",
    "train_df = pd.DataFrame(np.concatenate((train_images, train_labels), axis=1), columns=list(range(len(train_images[0]))) + [\"Class\"])\n",
    "test_df = pd.DataFrame(np.concatenate((test_images, test_labels), axis=1), columns=list(range(len(test_images[0]))) + [\"Class\"])\n",
    "train_stretched_df = pd.DataFrame(np.concatenate((train_stretched_images, train_labels), axis=1), columns=list(range(len(train_stretched_images[0]))) + [\"Class\"])\n",
    "test_stretched_df = pd.DataFrame(np.concatenate((test_stretched_images, test_labels), axis=1), columns=list(range(len(test_stretched_images[0]))) + [\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.a\n",
      "Normal:\n",
      " Untouched:\n",
      "  test_acc 0.7779\n",
      "  train_acc 0.7714\n",
      " stretched:\n",
      "  test_acc 0.7279\n",
      "  train_acc 0.7203333333333334\n",
      "Bernoulli:\n",
      " Untouched:\n",
      "  test_acc 0.8384\n",
      "  train_acc 0.8319166666666666\n",
      " stretched:\n",
      "  test_acc 0.8101\n",
      "  train_acc 0.7998166666666666\n"
     ]
    }
   ],
   "source": [
    "#p2.a\n",
    "#Use the NBC from problem 1\n",
    "print(\"2.a\")\n",
    "print(\"Normal:\")\n",
    "print(\" Untouched:\")\n",
    "print(\"  test_acc\", NBC(train_df = train_df, test_df = test_df, distribution=\"Normal\").train().predict().get_acc())\n",
    "print(\"  train_acc\", NBC(train_df = train_df, test_df = train_df, distribution=\"Normal\").train().predict().get_acc())\n",
    "print(\" Stretched:\")\n",
    "print(\"  test_acc\", NBC(train_df = train_stretched_df, test_df = test_stretched_df, distribution=\"Normal\").train().predict().get_acc())\n",
    "print(\"  train_acc\", NBC(train_df = train_stretched_df, test_df = train_stretched_df, distribution=\"Normal\").train().predict().get_acc())\n",
    "\n",
    "print(\"Bernoulli:\")\n",
    "print(\" Untouched:\")\n",
    "print(\"  test_acc\", NBC(train_df = train_df, test_df = test_df, distribution=\"Bernoulli\").train().predict().get_acc())\n",
    "print(\"  train_acc\", NBC(train_df = train_df, test_df = train_df, distribution=\"Bernoulli\").train().predict().get_acc())\n",
    "print(\" Stretched:\")\n",
    "print(\"  test_acc\", NBC(train_df = train_stretched_df, test_df = test_stretched_df, distribution=\"Bernoulli\").train().predict().get_acc())\n",
    "print(\"  train_acc\", NBC(train_df = train_stretched_df, test_df = train_stretched_df, distribution=\"Bernoulli\").train().predict().get_acc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.30258509])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
